"""
This module provides asynchronous functions to process both structured JSON chunks
and unstructured free‑form documents for a given medical case identifier (MCID).

There are two sources of information for each MCID:

* Normalised_Input_Jsons/<MCID>/<category>/chunk_*.txt – structured JSON input
  files divided into one or more chunks.  Each category (e.g. ``claims``,
  ``office_visits``) may have multiple ``chunk_*.txt`` files.  The
  ``chat_prompt_template`` used for JSON processing must include the
  ``{json_context}`` and ``{json_desc}`` placeholders.  A description file for
  each category is automatically loaded from ``desc_dir/<category>.txt`` and
  substituted into the ``{json_desc}`` placeholder.  The content of each
  ``chunk_*.txt`` file replaces ``{json_context}`` before calling the
  ``hc4sass.marshal_into_json`` service.

* Normalised_Input_Docs/<MCID>/*.txt – unstructured documents such as CCDA or
  patient summaries.  Each file is processed independently using
  ``doc_prompt_template`` which must contain a single ``{doc_context}``
  placeholder.  The entire contents of the document file are inserted into
  ``{doc_context}`` and sent to ``hc4sass.marshal_into_json`` with its own
  schema and parameter set.

Both sources are processed concurrently using ``asyncio``.  The returned
structure mirrors the input hierarchy: JSON categories map to dictionaries of
``chunk_n`` responses, while documents map to dictionaries keyed by the
document filename with a single ``chunk_n`` entry per file (the index is
assigned based on document order).  If no categories or documents are
available, a message indicating the absence of information is returned instead
of raising an exception.

These helpers do not perform any disk writes; they return Python objects
containing the marshalled responses.  Downstream callers can decide how
to persist or otherwise consume the data.

    Example usage::

    import asyncio
    from parallel_processing import marshal_json_and_docs

        combined = asyncio.run(marshal_json_and_docs(
            json_base_path="/path/to/Normalised_Input_Jsons/58192827",
            docs_base_path="/path/to/Normalised_Input_Docs/58192827",
            json_categories=["claims", "office_visits"],
            json_schema=chat_schema,
            json_params=params_4o,
            json_prompt_template=chat_prompt,  # must include {json_context} and {json_desc}
            # Description files live directly under the Jsons_Description folder,
            # independent of any MCID.  For example, claims.txt and office_visits.txt.
            desc_dir="/path/to/TMV_ChatBot_Final/Jsons_Description",
            doc_schema=chat_schema,
            doc_params=params_4o,
            doc_prompt_template=doc_prompt,    # must include {doc_context}
        ))

        # combined now contains a flat dictionary where category names and
        # document base names map to their chunked answers.  For example:
        # {
        #   "claims": {"chunk_1": {...}},
        #   "office_visits": {"chunk_1": {...}},
        #   "CCDA-2025-05-12_1": {"chunk_1": {...}},
        #   "Patient_Summary_2025-05-15": {"chunk_2": {...}}
        # }

Note:
    The ``hc4sass`` client must be available in the runtime environment with an
    asynchronous ``marshal_into_json`` method.  The functions defined here
    assume that ``hc4sass.marshal_into_json(...)`` accepts the same arguments
    used throughout this project (``schema``, ``prompt`` and ``params``) and
    returns a JSON‑serialisable Python object.
"""

from __future__ import annotations

import asyncio
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

try:
    # Attempt to import the hc4sass client; if unavailable, a stub can be
    # provided by the caller for testing purposes.  The stub should expose
    # ``marshal_into_json`` with the same signature used below.
    import hc4sass  # type: ignore
except ImportError:
    hc4sass = None  # type: ignore


# ---------------------------------------------------------------------------
# Helper functions
#
# These helpers perform sorting and description lookup.  They do not depend
# on any asynchronous functionality and thus can be used synchronously.
# ---------------------------------------------------------------------------

def _natural_key(s: str) -> List[Union[str, int]]:
    """Return a key for natural sorting (case‑insensitive and numeric aware).

    This splits a string into runs of digits and non‑digits so that 'file10'
    sorts after 'file2' instead of between 'file1' and 'file2'.
    """
    return [int(text) if text.isdigit() else text.lower() for text in re.findall(r"\d+|\D+", s)]


def _sorted_chunk_files(folder: Path) -> List[Tuple[Path, int]]:
    """Return a list of (file_path, chunk_index) for all ``chunk*.txt`` files.

    Files are detected using a regular expression on the filename.  The chunk
    number extracted from the filename determines ordering.  For example,
    ``chunk_1.txt`` yields index 1, and will appear before ``chunk_2.txt``.
    Unmatched files are ignored.  The return list is sorted by the numeric
    chunk index.
    """
    chunk_pattern = re.compile(r"^chunk[_-]?(\d+)\.txt$", re.IGNORECASE)
    files: List[Tuple[Path, int]] = []
    for fp in folder.rglob("chunk*.txt"):
        match = chunk_pattern.match(fp.name)
        if match:
            try:
                idx = int(match.group(1))
            except ValueError:
                continue
            files.append((fp, idx))
    files.sort(key=lambda t: t[1])
    return files


def _sorted_doc_files(folder: Path) -> List[Path]:
    """Return a list of document text files sorted naturally by name.

    Only files with a ``.txt`` suffix at the top level of ``folder`` are
    included.  Sorting is case‑insensitive and numeric aware.
    """
    if not folder.exists() or not folder.is_dir():
        return []
    files = [fp for fp in folder.iterdir() if fp.is_file() and fp.suffix.lower() == ".txt"]
    files.sort(key=lambda p: _natural_key(p.name))
    return files


def _load_category_description(desc_dir: Path, category: str) -> str:
    """Attempt to load a description file for a given category.

    First tries an exact match of ``<category>.txt``.  If no exact match
    exists, performs a case‑ and punctuation‑insensitive lookup where any
    non‑alphanumeric characters are replaced with underscores.  Returns an
    empty string if no matching description is found or if an error occurs.
    """
    # Exact match
    exact = desc_dir / f"{category}.txt"
    if exact.exists() and exact.is_file():
        try:
            return exact.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            return ""

    # Case/punctuation normalised match
    def normalise(name: str) -> str:
        return re.sub(r"[^a-z0-9]+", "_", name.lower()).strip("_")

    target = normalise(category)
    for fp in desc_dir.glob("*.txt"):
        if normalise(fp.stem) == target:
            try:
                return fp.read_text(encoding="utf-8", errors="ignore")
            except Exception:
                return ""
    return ""


# ---------------------------------------------------------------------------
# Asynchronous processing helpers
#
# These functions use asyncio to process multiple files concurrently.  They
# accept prompt templates with placeholders and yield dictionaries of results.
# ---------------------------------------------------------------------------

async def _marshal_text(
    text: str,
    schema: Any,
    params: Dict[str, Any],
    prompt_template: str,
    placeholder: str,
    sem: asyncio.Semaphore,
) -> Any:
    """Marshal a single text through the hc4sass client with concurrency control.

    The provided ``prompt_template`` must contain a named placeholder (e.g.
    ``{json_context}`` or ``{doc_context}``).  The placeholder name is passed
    via the ``placeholder`` argument and will be replaced with the text.  All
    other format fields must already be resolved in the template.  This helper
    returns the raw result from ``hc4sass.marshal_into_json``.  If the global
    ``hc4sass`` client is not available, an error string is returned instead.
    """
    async with sem:
        try:
            if hc4sass is None:
                return {"_error": "hc4sass client is unavailable"}
            prompt = prompt_template.format(**{placeholder: text})
            return await hc4sass.marshal_into_json(
                schema=schema,
                prompt=prompt,
                params=params,
            )
        except Exception as exc:
            # Catch any exception and return a structured error
            return {"_error": str(exc)}


async def _process_json_category(
    category_path: Path,
    chat_schema: Any,
    params_4o: Dict[str, Any],
    chat_prompt_template: str,
    desc_text: str,
    max_concurrency: int,
) -> Dict[str, Any]:
    """Process all chunk files under a single JSON category folder.

    Loads each ``chunk*.txt`` file, fills the ``{json_context}`` placeholder
    with its content and the ``{json_desc}`` placeholder with ``desc_text``,
    then calls the ``hc4sass.marshal_into_json`` service asynchronously.  The
    results are gathered into a dictionary keyed by ``chunk_<index>``.
    """
    # Preformat the prompt with the description once to avoid repeated work.
    if "{json_desc}" not in chat_prompt_template:
        raise ValueError("chat_prompt_template must contain {json_desc}")
    if "{json_context}" not in chat_prompt_template:
        raise ValueError("chat_prompt_template must contain {json_context}")

    prompt_with_desc = chat_prompt_template.format(json_desc=desc_text)
    sem = asyncio.Semaphore(max_concurrency)
    tasks: List[asyncio.Task] = []
    chunk_files = _sorted_chunk_files(category_path)

    for fp, idx in chunk_files:
        try:
            text = fp.read_text(encoding="utf-8", errors="ignore")
        except Exception as exc:
            # If the file cannot be read, record an error immediately
            tasks.append(asyncio.create_task(asyncio.sleep(0, {"_error": f"Failed to read {fp}: {exc}"})))
            continue
        tasks.append(
            asyncio.create_task(
                _marshal_text(
                    text=text,
                    schema=chat_schema,
                    params=params_4o,
                    prompt_template=prompt_with_desc,
                    placeholder="json_context",
                    sem=sem,
                )
            )
        )

    results: Dict[str, Any] = {}
    gathered = await asyncio.gather(*tasks, return_exceptions=False)
    for (fp, idx), res in zip(chunk_files, gathered):
        key = f"chunk_{idx}"
        # If res is a coroutine exception, it will be captured above; no need to
        # distinguish here.  We simply assign the result as is.
        results[key] = res
    return results


async def _process_document_files(
    docs_path: Path,
    doc_schema: Any,
    doc_params: Dict[str, Any],
    doc_prompt_template: str,
    max_concurrency: int,
) -> Dict[str, Any]:
    """Process each document file under ``docs_path`` with its own chunk key.

    Each ``.txt`` file in the folder is treated as a single chunk.  The
    ``doc_prompt_template`` must contain the ``{doc_context}`` placeholder
    which will be replaced by the file content.  The results are grouped in
    a dictionary keyed by the document filename; within each entry, the key
    ``chunk_<n>`` is assigned according to the document's order (1‑based).
    """
    if "{doc_context}" not in doc_prompt_template:
        raise ValueError("doc_prompt_template must contain {doc_context}")

    sem = asyncio.Semaphore(max_concurrency)
    tasks: List[asyncio.Task] = []
    doc_files = _sorted_doc_files(docs_path)
    # We assign a global chunk index across all documents starting from 1.
    # Each document will get its own index.  If a document fails to read,
    # an error entry is returned for that document.
    for fp in doc_files:
        try:
            text = fp.read_text(encoding="utf-8", errors="ignore")
        except Exception as exc:
            tasks.append(asyncio.create_task(asyncio.sleep(0, {"_error": f"Failed to read {fp}: {exc}"})))
            continue
        tasks.append(
            asyncio.create_task(
                _marshal_text(
                    text=text,
                    schema=doc_schema,
                    params=doc_params,
                    prompt_template=doc_prompt_template,
                    placeholder="doc_context",
                    sem=sem,
                )
            )
        )

    results: Dict[str, Any] = {}
    gathered = await asyncio.gather(*tasks, return_exceptions=False)
    for idx, (fp, res) in enumerate(zip(doc_files, gathered), start=1):
        # Use the document filename as the top‑level key.  Within that, assign
        # a single chunk key "chunk_<n>" based on the document's order.
        results.setdefault(fp.name, {})[f"chunk_{idx}"] = res
    return results


async def _marshal_json_and_docs_with_mcid(
    mcid: str,
    json_base_path: Union[str, Path],
    docs_base_path: Union[str, Path],
    json_categories: List[str],
    json_schema: Any,
    json_params: Dict[str, Any],
    json_prompt_template: str,
    desc_dir: Union[str, Path],
    doc_schema: Any,
    doc_params: Dict[str, Any],
    doc_prompt_template: str,
    json_concurrency: int = 8,
    doc_concurrency: int = 8,
) -> Dict[str, Any]:
    """(Deprecated) Process JSON categories and document files concurrently for a given MCID.

    This helper preserves backward compatibility with the previous API.  It
    expects an MCID argument and returns the results nested under ``json`` and
    ``documents`` keys along with the MCID.  It should not be used for new
    development; use :func:`marshal_json_and_docs` instead.
    """
    json_base = Path(json_base_path)
    docs_base = Path(docs_base_path)
    desc_dir_path = Path(desc_dir)

    async def process_categories() -> Dict[str, Any]:
        if not json_categories:
            return {"message": "Information is not available."}
        tasks: List[asyncio.Task] = []
        keys: List[str] = []
        for category in json_categories:
            cat_path = json_base / category
            if not cat_path.exists() or not cat_path.is_dir():
                keys.append(category)
                tasks.append(asyncio.create_task(asyncio.sleep(0, {"_error": f"Category path not found: {cat_path}"})))
                continue
            desc_text = _load_category_description(desc_dir_path, category)
            keys.append(category)
            tasks.append(
                asyncio.create_task(
                    _process_json_category(
                        category_path=cat_path,
                        chat_schema=json_schema,
                        params_4o=json_params,
                        chat_prompt_template=json_prompt_template,
                        desc_text=desc_text,
                        max_concurrency=json_concurrency,
                    )
                )
            )
        results: Dict[str, Any] = {}
        gathered = await asyncio.gather(*tasks, return_exceptions=False)
        for name, res in zip(keys, gathered):
            results[name] = res
        return results

    async def process_docs() -> Dict[str, Any]:
        if not docs_base.exists() or not docs_base.is_dir():
            return {"message": "Information is not available."}
        return await _process_document_files(
            docs_path=docs_base,
            doc_schema=doc_schema,
            doc_params=doc_params,
            doc_prompt_template=doc_prompt_template,
            max_concurrency=doc_concurrency,
        )

    json_task = asyncio.create_task(process_categories())
    docs_task = asyncio.create_task(process_docs())
    json_results, docs_results = await asyncio.gather(json_task, docs_task)
    return {
        "mcid": mcid,
        "json": json_results,
        "documents": docs_results,
    }


async def marshal_json_and_docs(
    json_base_path: Union[str, Path],
    docs_base_path: Union[str, Path],
    json_categories: List[str],
    json_schema: Any,
    json_params: Dict[str, Any],
    json_prompt_template: str,
    desc_dir: Union[str, Path],
    doc_schema: Any,
    doc_params: Dict[str, Any],
    doc_prompt_template: str,
    json_concurrency: int = 8,
    doc_concurrency: int = 8,
) -> Dict[str, Any]:
    """Process JSON categories and document files concurrently and return a flat dict.

    Parameters
    ----------
    json_base_path: str or Path
        Path to the directory containing category subdirectories, e.g.
        ``.../Normalised_Input_Jsons/<MCID>``.
    docs_base_path: str or Path
        Path to the directory containing free‑form document files for the same
        case, e.g. ``.../Normalised_Input_Docs/<MCID>``.
    json_categories: List[str]
        A list of category names (subdirectory names under ``json_base_path``)
        to process.  If the list is empty, no JSON categories will be
        processed and their results will not be present in the output.
    json_schema: Any
        Schema passed directly to ``hc4sass.marshal_into_json`` for JSON
        category processing.
    json_params: Dict[str, Any]
        Parameters passed directly to ``hc4sass.marshal_into_json`` for JSON
        category processing.
    json_prompt_template: str
        Prompt template used for JSON categories.  Must contain
        ``{json_context}`` and ``{json_desc}`` placeholders.
    desc_dir: str or Path
        Directory containing category description files named
        ``<category>.txt``.  The description is substituted into
        ``{json_desc}`` for each category.  If the category description is
        missing, an empty string is used.
    doc_schema: Any
        Schema passed directly to ``hc4sass.marshal_into_json`` for document
        processing.
    doc_params: Dict[str, Any]
        Parameters passed directly to ``hc4sass.marshal_into_json`` for
        document processing.
    doc_prompt_template: str
        Prompt template used for documents.  Must contain the
        ``{doc_context}`` placeholder.  The full file content will be
        substituted into this placeholder.  No description is used for
        documents.
    json_concurrency: int, optional
        Maximum concurrency for chunk processing in each JSON category.
        Defaults to 8.
    doc_concurrency: int, optional
        Maximum concurrency for document processing.  Defaults to 8.

    Returns
    -------
    Dict[str, Any]
        A flat dictionary mapping category names and document base names
        (without the ``.txt`` suffix) to dictionaries of chunk results.
        For example::

            {
                "claims": {"chunk_1": {...}},
                "office_visits": {"chunk_1": {...}},
                "CCDA-2025-05-12_1": {"chunk_1": {...}},
                "Patient_Summary_2025-05-15": {"chunk_2": {...}},
                ...
            }

        If neither JSON categories nor documents yield any results, the
        returned dictionary will contain a single key ``"message"`` with the
        value ``"Information is not available."``.
    """
    json_base = Path(json_base_path)
    docs_base = Path(docs_base_path)
    desc_dir_path = Path(desc_dir)

    # Prepare JSON category tasks
    category_tasks: List[asyncio.Task] = []
    category_names: List[str] = []
    if json_categories:
        for category in json_categories:
            cat_path = json_base / category
            if not cat_path.exists() or not cat_path.is_dir():
                # Skip missing categories by inserting an error result
                category_names.append(category)
                category_tasks.append(asyncio.create_task(asyncio.sleep(0, {"_error": f"Category path not found: {cat_path}"})))
                continue
            desc_text = _load_category_description(desc_dir_path, category)
            category_names.append(category)
            category_tasks.append(
                asyncio.create_task(
                    _process_json_category(
                        category_path=cat_path,
                        chat_schema=json_schema,
                        params_4o=json_params,
                        chat_prompt_template=json_prompt_template,
                        desc_text=desc_text,
                        max_concurrency=json_concurrency,
                    )
                )
            )

    # Prepare document tasks (single call handles all docs)
    async def get_docs_results() -> Dict[str, Any]:
        if not docs_base.exists() or not docs_base.is_dir():
            return {}
        return await _process_document_files(
            docs_path=docs_base,
            doc_schema=doc_schema,
            doc_params=doc_params,
            doc_prompt_template=doc_prompt_template,
            max_concurrency=doc_concurrency,
        )

    # Run JSON tasks and document processing concurrently
    if category_tasks:
        json_results_list = await asyncio.gather(*category_tasks, return_exceptions=False)
    else:
        json_results_list = []
    docs_task = asyncio.create_task(get_docs_results())
    docs_results = await docs_task

    combined: Dict[str, Any] = {}
    # Combine JSON results
    for name, res in zip(category_names, json_results_list):
        combined[name] = res
    # Combine document results (rename keys and preserve chunk indices)
    if docs_results:
        for doc_name, chunk_dict in docs_results.items():
            # Remove suffix and replace spaces with underscores
            base_name = Path(doc_name).stem.replace(" ", "_")
            combined[base_name] = chunk_dict

    if not combined:
        return {"message": "Information is not available."}
    return combined


__all__ = [
    "marshal_json_and_docs",
    "_marshal_json_and_docs_with_mcid",
    "_process_json_category",
    "_process_document_files",
    "_load_category_description",
    "_sorted_chunk_files",
    "_sorted_doc_files",
]
