Here’s the revised version without using “I”:

**Hybrid MRR Chatbot:**
As per the previous meeting’s discussion, around 1,200 questions were tested (20 files with 71 questions each). Outputs were documented using a framework that captures each step’s result along with time consumption for every process. The analysis includes guardrail accuracy and metrics for validating outputs in line with responsible AI practices.

**Next Step:**
Although the files tested were randomly selected and the context size was within the LLM’s context window, the next step is to test larger files with human evaluation.

Let me know if you'd like this further refined.
