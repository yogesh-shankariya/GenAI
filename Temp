import re
from datetime import datetime
from pathlib import Path
import pandas as pd
import numpy as np


def parse_docmeta_log_accurate(
    log_source,
    start_marker_regex=r"in\s+main_test:\s+\*{5,}",     # matches: in main_test: ***************
    ts_regex=r"^\[(?P<ts>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3})\]",
    ts_fmt="%Y-%m-%d %H:%M:%S,%f"
):
    """
    1) Split the log into files using the exact 'main_test: ********' marker.
    2) Compute time-taken per file as: start_of_this_main_test  ->  start_of_next_main_test
       (last file uses its last timestamp).
    3) Return ERROR and WARNING counts grouped by (file_seq, where, message).

    Parameters
    ----------
    log_source : str | Path
        Path to the log file or raw log text.
    start_marker_regex : str
        Regex to match the 'start of file' marker.
    ts_regex : str
        Regex that captures timestamp at start of each log line.
    ts_fmt : str
        Datetime format for timestamps.

    Returns
    -------
    per_file_df : DataFrame
    summary_df  : DataFrame
    error_df    : DataFrame
    warning_df  : DataFrame
    """

    # --- read -----------------------------------------------------------------
    if isinstance(log_source, (str, Path)) and Path(str(log_source)).exists():
        text = Path(str(log_source)).read_text(encoding="utf-8", errors="ignore")
    else:
        text = str(log_source)

    lines = text.splitlines()

    # --- regexes --------------------------------------------------------------
    ts_pat = re.compile(ts_regex)
    start_pat = re.compile(start_marker_regex)
    s3_pat = re.compile(r"Received binary S3 path:\s*([^\s]+)")
    line_pat = re.compile(
        r"^\[(?P<ts>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3})\]\s+"
        r"(?P<level>INFO|WARNING|ERROR)\s+\[[^\]]+\]\s+in\s+"
        r"(?P<where>[^:]+):\s+(?P<msg>.*)$"
    )

    # --- find all timestamps once (speed & simplicity) ------------------------
    parsed_lines = []
    for idx, line in enumerate(lines):
        m = ts_pat.match(line)
        if not m:
            parsed_lines.append({"idx": idx, "ts": None, "line": line})
            continue
        ts = datetime.strptime(m.group("ts"), ts_fmt)
        parsed_lines.append({"idx": idx, "ts": ts, "line": line})

    # --- locate start markers --------------------------------------------------
    markers = []
    for item in parsed_lines:
        if item["ts"] is None:
            continue
        if start_pat.search(item["line"]):
            path_m = s3_pat.search(item["line"])
            markers.append({
                "idx": item["idx"],
                "ts": item["ts"],
                "s3_path": path_m.group(1) if path_m else None
            })

    if not markers:
        raise ValueError("No 'main_test' markers found. Check start_marker_regex.")

    # --- build per-file blocks using NEXT marker's ts as end -------------------
    blocks = []
    for i, mk in enumerate(markers):
        start_ts = mk["ts"]
        start_idx = mk["idx"]
        s3_path = mk["s3_path"]

        if i < len(markers) - 1:
            end_ts = markers[i + 1]["ts"]
            end_idx = markers[i + 1]["idx"] - 1
        else:
            # last block: end at last timestamp in file
            last_ts = next(
                (p["ts"] for p in reversed(parsed_lines) if p["ts"] is not None),
                start_ts
            )
            end_ts = last_ts
            end_idx = len(lines) - 1

        blocks.append({
            "file_seq": i + 1,
            "s3_path": s3_path,
            "start_time": start_ts,
            "end_time": end_ts,
            "start_idx": start_idx,
            "end_idx": end_idx
        })

    per_file_rows = []
    # map line index -> file_seq for quick lookup later
    idx_to_seq = {}

    for b in blocks:
        dur = (b["end_time"] - b["start_time"]).total_seconds()
        per_file_rows.append({
            "file_seq": b["file_seq"],
            "s3_path": b["s3_path"],
            "start_time": b["start_time"],
            "end_time": b["end_time"],
            "time_taken_for_execution": dur
        })
        for li in range(b["start_idx"], b["end_idx"] + 1):
            idx_to_seq[li] = b["file_seq"]

    per_file_df = pd.DataFrame(per_file_rows)

    # --- summary stats ---------------------------------------------------------
    if per_file_df.empty:
        metrics = {k: None for k in [
            "min_execution_time", "max_execution_time", "average_execution_time",
            "percentile_25_value", "percentile_50_value", "percentile_75_value", "percentile_99_value"
        ]}
    else:
        t = per_file_df["time_taken_for_execution"].to_numpy()
        metrics = {
            "min_execution_time": t.min(),
            "max_execution_time": t.max(),
            "average_execution_time": t.mean(),
            "percentile_25_value": np.percentile(t, 25),
            "percentile_50_value": np.percentile(t, 50),
            "percentile_75_value": np.percentile(t, 75),
            "percentile_99_value": np.percentile(t, 99)
        }
    summary_df = pd.DataFrame([metrics]).T
    summary_df.columns = ["value"]

    # --- ERROR / WARNING extraction -------------------------------------------
    ew_rows = []
    for item in parsed_lines:
        line = item["line"]
        m = line_pat.match(line)
        if not m:
            continue

        lvl = m.group("level")
        if lvl not in ("ERROR", "WARNING"):
            continue

        line_idx = item["idx"]
        file_seq = idx_to_seq.get(line_idx, None)

        ew_rows.append({
            "ts": item["ts"],
            "level": lvl,
            "where": m.group("where").strip(),
            "message": m.group("msg").strip(),
            "file_seq": file_seq
        })

    ew_df = pd.DataFrame(ew_rows)

    if ew_df.empty:
        error_df = pd.DataFrame(columns=["file_seq", "where", "message", "count", "first_seen", "last_seen"])
        warning_df = error_df.copy()
    else:
        error_df = (
            ew_df[ew_df.level == "ERROR"]
            .groupby(["file_seq", "where", "message"], dropna=False)
            .agg(count=("message", "size"),
                 first_seen=("ts", "min"),
                 last_seen=("ts", "max"))
            .reset_index()
            .sort_values(["file_seq", "count"], ascending=[True, False])
        )

        warning_df = (
            ew_df[ew_df.level == "WARNING"]
            .groupby(["file_seq", "where", "message"], dropna=False)
            .agg(count=("message", "size"),
                 first_seen=("ts", "min"),
                 last_seen=("ts", "max"))
            .reset_index()
            .sort_values(["file_seq", "count"], ascending=[True, False])
        )

    return per_file_df, summary_df, error_df, warning_df
