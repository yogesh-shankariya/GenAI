"""
Ingestion pipeline for LangChain FAISS vector stores.

This script reads healthcare JSON files from a directory, converts
each record into a LangChain ``Document`` object with associated
metadata (including ``mcid`` and ``json_name``), computes dense
embeddings using a HuggingFace model (default ``BAAI/bge-m3``), and
builds a FAISS index for each JSON file using LangChain's
``FAISS.from_documents`` helper.  Each index can optionally be
persisted to disk.

Features:
    - MCID tagging: assign an ``mcid`` value to each record's metadata.
    - Configurable embedding model: choose any HuggingFace model via
      ``--model-name`` (defaults to ``BAAI/bge-m3``).
    - Optional index saving: specify ``--save-dir`` to save each
      collection's FAISS index under that directory.  Indices can
      later be reloaded with ``vector_store.load_store``.

Example usage::

    python ingestion_pipeline.py \
        --data-dir /path/to/jsons \
        --mcid ABC123 \
        --model-name BAAI/bge-m3 \
        --save-dir faiss_indices

Notes:
    - This script uses LangChain's HuggingFaceEmbeddings, which
      requires the ``sentence_transformers`` package.  Ensure
      ``langchain-community`` and ``faiss-cpu`` are installed in
      your environment.
    - The user must have the rights to download the specified model
      from HuggingFace or have it cached locally.
"""

from __future__ import annotations

import argparse
from pathlib import Path
from typing import Optional

from data_preprocessing import iter_dataset
from embedding import create_embedder, HFEmbedder
from vector_store import (
    create_faiss_store_from_documents,
    save_store,
)

import shutil

try:
    from langchain.docstore.document import Document
except ImportError as e:
    raise ImportError(
        "LangChain is required to run the ingestion pipeline. "
        "Please install langchain-community and sentence-transformers."
    ) from e


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Ingest JSON records into FAISS vector stores using LangChain.")
    parser.add_argument("--data-dir", type=str, required=True, help="Directory containing JSON files to ingest")
    parser.add_argument("--mcid", type=str, required=True, help="MCID value to assign to all records")
    parser.add_argument("--model-name", type=str, default="BAAI/bge-m3", help="HuggingFace model name for embeddings")
    parser.add_argument("--save-dir", type=str, default=None, help="Directory to save each FAISS index. If omitted, indices are not persisted.")
    parser.add_argument("--model-device", type=str, default="cpu", help="Device for embedding model, e.g. 'cpu' or 'cuda'")
    parser.add_argument("--encode-normalize", action="store_true", help="Whether to normalise embeddings during encoding")
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    data_dir = Path(args.data_dir)
    if not data_dir.is_dir():
        raise FileNotFoundError(f"Data directory {data_dir} does not exist")

    # Prepare the embedder. Normalisation is optional; leaving it
    # disabled can improve recall for some retrieval tasks. Device
    # selection can be controlled via command line.
    model_kwargs = {"device": args.model_device}
    encode_kwargs = {"normalize_embeddings": args.encode_normalize}
    base_embedder = create_embedder(
        model_name=args.model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs,
    )
    embedder = HFEmbedder(base_embedder)

    # Accumulate a combined store to merge all per-file indexes
    combined_store = None  # type: ignore
    # Keep track of subfolders we create for per-file indexes
    created_subfolders: list[Path] = []

    # Iterate over each supported JSON file and process records
    for collection_name, file_path, records in iter_dataset(data_dir, mcid=args.mcid):
        if not records:
            continue
        print(f"Processing {file_path} with {len(records)} records...")
        # Convert records into Document objects.  The Document class
        # expects 'page_content' and 'metadata'.
        documents = []
        for rec_id, text, meta in records:
            # Store the record ID as part of metadata so it can be
            # retrieved later when querying.
            doc = Document(page_content=text, metadata={**meta, "record_id": rec_id})
            documents.append(doc)
        # Build a FAISS index for this collection
        store = create_faiss_store_from_documents(documents, embedder)
        print(f"Built FAISS index for collection '{collection_name}'.")

        # Merge into combined store
        if combined_store is None:
            combined_store = store
        else:
            combined_store.merge_from(store)  # type: ignore

        # If a save directory is provided, persist the index to disk
        if args.save_dir:
            save_path = Path(args.save_dir) / collection_name
            save_path.parent.mkdir(parents=True, exist_ok=True)
            save_store(store, str(save_path))
            created_subfolders.append(save_path)
            print(f"Saved index for '{collection_name}' to {save_path}.")

    # After processing all files, save the combined store and delete subfolders
    if combined_store is not None:
        if args.save_dir:
            # Define final combined path
            combined_path = Path(args.save_dir) / f"combined_{args.mcid}"
            combined_path.parent.mkdir(parents=True, exist_ok=True)
            save_store(combined_store, str(combined_path))
            print(f"Saved combined index to {combined_path}.")

            # Remove individual subfolders after saving combined
            removed = 0
            for subfolder in created_subfolders:
                try:
                    shutil.rmtree(subfolder)
                    removed += 1
                except Exception as exc:
                    print(f"Warning: failed to remove {subfolder}: {exc}")
            if removed:
                print(f"Deleted {removed} per-file index folder(s) in {args.save_dir}.")

    print("Ingestion complete.")


if __name__ == "__main__":
    main()
