Below are the **only two edits** you need to launch the fourth job—**question-recommendation**—in parallel with your existing three tasks.

---

## 1️⃣  LLMClient – make question-recommendation await-able

```python
# llm_client.py
class LLMClient:
    # ─────────────────────────────────────────────────────────────
    #  synchronous implementation renamed (no logic changes)
    # ─────────────────────────────────────────────────────────────
    def _sync_get_question_recommendation(self, conversation: str) -> dict:
        authToken = getAuthToken(
            Constant.HORIZON_CLIENT_ID,
            Constant.HORIZON_CLIENT_SECRET,
            Constant.HORIZON_GATEWAY,
        )
        if not authToken:
            self.logger.error("Recommendation : Authentication Failed")
            return {}

        messages = [
            {"role": "system", "content": question_recommendation_prompt["system_prompt"]},
            {
                "role": "user",
                "content": question_recommendation_prompt["user_prompt"].format(
                    conversation=conversation
                ),
            },
        ]

        completion = self.chats(
            authToken=authToken,
            address=Constant.HORIZON_GATEWAY,
            messages=messages,
        )
        if completion:
            self.logger.info("Got response from Recommendation LLM.")
            return json.loads(completion.text)["message"]["content"]

        self.logger.error("No response received from Recommendation LLM.")
        return {}

    # ─────────────────────────────────────────────────────────────
    #  thin async façade – now await-able
    # ─────────────────────────────────────────────────────────────
    async def get_question_recommendation(self, conversation: str) -> dict:
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(
            None,                          # default ThreadPoolExecutor
            self._sync_get_question_recommendation,
            conversation,
        )
```

Nothing else inside the method body changes; retry logic, logging, and parsing stay as-is.

---

## 2️⃣  Chatbot – schedule the 4th coroutine in `gather`

Inside your *\_process\_request\_async* helper (or wherever you already build the three tasks):

```python
# 1. raw answer first
response, status = await self.llm_client.execute_chat_with_guardrail(
    messages, user_question, history
)

# 2. launch all four independent jobs
structured_task = asyncio.create_task(
    self.llm_client.get_structured_answer(response)
)
faith_task = asyncio.create_task(
    self.ResponseValidationScoreCalculator.get_faithfulness_score(
        answer=response,
        context=file_content,
    )
)
rel_task = asyncio.create_task(
    self.ResponseValidationScoreCalculator.get_relevance_score(
        answer=response,
        context=file_content,
        question=question_with_history,
    )
)
recommend_task = asyncio.create_task(
    self.llm_client.get_question_recommendation(conversation=question_with_history)
)

structured_ans, faithfulness_score, relevance_score, recommended_qs = await asyncio.gather(
    structured_task, faith_task, rel_task, recommend_task
)

# 3. build final payload
final_json = {
    "status": 200,
    "data": {
        "content": structured_ans.get("content"),
        "reference": structured_ans.get("reference"),
        "metrics": {
            "faithfulness": faithfulness_score,
            "relevance":   relevance_score,
        },
        "recommended_questions": recommended_qs,   # ← new field
    },
}
return json.dumps(final_json)
```

---

### That’s it

* **API route stays synchronous** (`def chat_bot(...)`), still calling `chat.process_request(...)`.
* The four heavy LLM calls are genuinely concurrent; total latency ≈ slowest single call.
* No other modules need changing.

Let me know if you’d like logging/timeout safeguards or a quick benchmark snippet!
