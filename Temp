from pathlib import Path
import json
from datetime import datetime
from typing import Dict


def combine_page_jsons(
    input_root: str | Path = "Output_Final/HELIX",
    combined_root: str | Path = "Combined_Output/HELIX",
    combined_name: str = "combined.json",
    error_log: str | Path = "combine_errors.txt",
    overwrite: bool = True,
) -> Dict[Path, Path]:
    """
    Merge page-*.json files into one combined.json per PDF folder.

    • Keeps only the `"section_headers"` list under each page key.
    • Any exception (file missing, bad JSON, etc.) is logged and the loop
      continues; other PDFs are not affected.
    """
    input_root    = Path(input_root).expanduser().resolve()
    combined_root = Path(combined_root).expanduser().resolve()
    error_log     = Path(error_log).expanduser().resolve()

    combined_root.mkdir(parents=True, exist_ok=True)
    error_log.parent.mkdir(parents=True, exist_ok=True)

    combined_paths: Dict[Path, Path] = {}

    def log(msg: str):
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with error_log.open("a", encoding="utf-8") as fp:
            fp.write(f"[{ts}] {msg}\n")

    # Identify every directory that has at least one page-*.json
    pdf_dirs = [
        p for p in input_root.rglob("*")
        if p.is_dir() and any(f.name.startswith("page-") and f.suffix == ".json"
                              for f in p.iterdir())
    ]

    for pdf_dir in pdf_dirs:
        try:
            combined_path = (
                combined_root / pdf_dir.relative_to(input_root) / combined_name
            )

            if combined_path.exists() and not overwrite:
                print(f"⏭  Exists, skipping: {combined_path}")
                continue

            merged: Dict[str, list] = {}
            for page_file in sorted(pdf_dir.glob("page-*.json")):
                page_key = page_file.stem  # "page-1"
                try:
                    data = json.loads(page_file.read_text(encoding="utf-8"))
                except Exception as e:
                    log(f"Bad JSON {page_file}: {e}")
                    continue

                headers = data.get("section_headers", [])
                merged[page_key] = headers

            combined_path.parent.mkdir(parents=True, exist_ok=True)
            combined_path.write_text(
                json.dumps(merged, ensure_ascii=False, indent=2),
                encoding="utf-8"
            )
            combined_paths[pdf_dir] = combined_path
            print(f"✓  {pdf_dir.relative_to(input_root)} -> {combined_path}")

        except Exception as e:
            # Any unexpected error for this PDF folder
            log(f"Fatal error in {pdf_dir}: {type(e).__name__}: {e}")

    if combined_paths:
        print("\nFinished. Any issues were logged to:", error_log)
    else:
        print("No PDF folders processed.")
    return combined_paths


# ---------- example ----------
# combine_page_jsons()
