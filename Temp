"""
Build one combined FAISS index from all JSON files,
then save it and (optionally) remove old per-JSON sub-folders.

Example:
  python ingestion_pipeline.py \
      --data-dir Input/74380480 \
      --mcid 74380480 \
      --model-name huggingface_embedding_model \
      --save-dir faiss_indices
"""

from __future__ import annotations
import argparse
import shutil
from pathlib import Path
from typing import Optional, List

from data_preprocessing import iter_dataset
from embedding import create_embedder, HFEmbedder
from vector_store import (
    create_faiss_store_from_documents,
    save_store,
)
from langchain.docstore.document import Document
from langchain_community.vectorstores.faiss import FAISS


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser("Ingest all JSONs â†’ ONE FAISS store")
    p.add_argument("--data-dir", required=True, help="Folder with JSON files")
    p.add_argument("--mcid", required=True, help="MCID tag for metadata")
    p.add_argument("--model-name", default="BAAI/bge-m3")
    p.add_argument("--model-device", default="cpu")
    p.add_argument("--encode-normalize", action="store_true")
    p.add_argument("--save-dir", required=True, help="Where to save the final FAISS store")
    return p.parse_args()


def build_document_list(records) -> List[Document]:
    docs = []
    for rec_id, text, meta in records:
        docs.append(Document(page_content=text, metadata={**meta, "record_id": rec_id}))
    return docs


def main() -> None:
    args = parse_args()
    data_dir = Path(args.data_dir)
    if not data_dir.is_dir():
        raise FileNotFoundError(data_dir)

    # Embedder
    embedder = HFEmbedder(
        create_embedder(
            model_name=args.model_name,
            model_kwargs={"device": args.model_device},
            encode_kwargs={"normalize_embeddings": args.encode_normalize},
        )
    )

    combined_store: Optional[FAISS] = None
    total_docs = 0

    # --- iterate every JSON file ---
    for _, file_path, records in iter_dataset(data_dir, mcid=args.mcid):
        if not records:
            continue

        docs = build_document_list(records)
        total_docs += len(docs)
        print(f"Processed {len(docs):>5} docs from {file_path}")

        # Build store for this file and merge into combined
        store = create_faiss_store_from_documents(docs, embedder)
        if combined_store is None:
            combined_store = store
        else:
            combined_store.merge_from(store)

    if combined_store is None:
        print("No supported JSON files found.")
        return

    # --- save single combined index ---
    save_root = Path(args.save_dir)
    save_root.mkdir(parents=True, exist_ok=True)
    combined_path = save_root / f"combined_{args.mcid}"
    combined_path.mkdir(exist_ok=True)

    save_store(combined_store, str(combined_path))
    print(f"\nSaved combined FAISS index ({total_docs} docs) to {combined_path}")

    # --- delete any per-JSON sub-folders that may exist ---
    removed = 0
    for item in save_root.iterdir():
        if item.is_dir() and item != combined_path:
            shutil.rmtree(item, ignore_errors=True)
            removed += 1
    if removed:
        print(f"Deleted {removed} old per-JSON sub-folders in {save_root}")

    print("Done.")


if __name__ == "__main__":
    main()
