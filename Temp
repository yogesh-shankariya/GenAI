"""
Chatbot – orchestrates file loading, token gating, page-chunking and parallel
query / guard-rail execution.
"""

from __future__ import annotations

import asyncio
import json
import os
import re
import traceback
from pathlib import Path
from typing import Optional, List, Dictimport json
import os
import asyncio
from typing import Any, Dict

# ---------- initialise once, e.g. at module import --------------------------
STEP_LOG_PATH: str = "step_outputs.json"
step_outputs: Dict[str, Any] = {}
step_lock: asyncio.Lock = asyncio.Lock()

# ------------- utility to load existing data on startup ---------------------
def _load_existing_steps() -> None:
    global step_outputs
    if os.path.isfile(STEP_LOG_PATH):
        with open(STEP_LOG_PATH, "r", encoding="utf-8") as fh:
            try:
                step_outputs = json.load(fh)
            except json.JSONDecodeError:
                # corrupted file → start fresh
                step_outputs = {}

# call once at startup
_load_existing_steps()

# --------------------------------------------------------------------------- #
async def record_step_output(step_number: int, output: Any) -> None:
    """
    Store or update an entry in `step_outputs.json` with proper indentation.

    Example call:
        await record_step_output(1, {"status": 200, "data": "hello"})

    Result in file:
        {
          "step-1": {
            "status": 200,
            "data": "hello"
          },
          ...
        }
    """
    key = f"step-{step_number}"

    async with step_lock:                # guarantees async-safe writes
        step_outputs[key] = output       # update in-memory dict
        # write atomically
        tmp_path = STEP_LOG_PATH + ".tmp"
        with open(tmp_path, "w", encoding="utf-8") as fh:
            json.dump(step_outputs, fh, indent=2, ensure_ascii=False)
        os.replace(tmp_path, STEP_LOG_PATH)


from chatbot.src.utils.config_loader import load_config
from chatbot.src.utils.prompt_loader import load_prompt
from chatbot.src.utils.logger import setup_logger
from chatbot.src.services.llm_client import LLMClient
from chatbot.src.services.responsible_ai import ResponseValidationScoreCalculator
from chatbot.src.utils.file_and_session_manager import FileHandler
from chatbot.src.utils.sql_db import (
    get_chat_history,
    insert_conversation,
    session_exists,
    create_conversation,
)
from chatbot.src.utils.tiktoken import count_tokens_for_context

# path to Q&A prompt template
INPUT_QA_PROMPT_PATH = os.path.join("prompts", "other", "input_prompt.yaml")

# regex for page tags
PAGE_TAG_RE = re.compile(r"<ocr_service_page_start>\s*(\d+)", re.I)


# ═════════════════════════════════════════════════════════════════════════════
class Chatbot:
# ═════════════════════════════════════════════════════════════════════════════
    def __init__(self, session_id: str):
        try:
            cfg                = load_config()
            self.session_id    = session_id
            self.logger        = setup_logger(cfg["logging"], session_id)

            create_conversation()

            self.llm_client    = LLMClient(cfg, self.logger)
            self.file_handler  = FileHandler(self.logger)

            prm                = load_prompt(INPUT_QA_PROMPT_PATH)
            self.SYS_TPL       = prm["system_prompt"]
            self.USER_TPL      = prm["user_prompt_template"]

            self.score_calc    = ResponseValidationScoreCalculator(cfg, self.logger)

        except Exception as exc:
            print(traceback.format_exc())
            raise RuntimeError(f"Init failed: {exc}") from exc

    # --------------------------------------------------------------------- #
    #  Helper: split OCR text into overlapping page-chunks
    # --------------------------------------------------------------------- #
    def _chunk_context(
        self, full_text: str, pages_per_chunk: int = 50, overlap: int = 20
    ) -> List[str]:
        positions: List[tuple[int, int]] = []
        for m in PAGE_TAG_RE.finditer(full_text):
            positions.append((int(m.group(1)), m.start()))
        positions.append((positions[-1][0] + 1, len(full_text)))  # sentinel

        # page → (start, end)
        page_slice: Dict[int, tuple[int, int]] = {
            p: (start, positions[i + 1][1]) for i, (p, start) in enumerate(positions[:-1])
        }
        max_page = max(page_slice)

        step = pages_per_chunk - overlap
        start_page = 1
        chunks = []
        while start_page <= max_page:
            end_page = start_page + pages_per_chunk - 1
            chunk_start = page_slice[start_page][0]
            chunk_end   = page_slice[min(end_page, max_page)][1]
            chunks.append(full_text[chunk_start:chunk_end])
            start_page += step
        return chunks

    # --------------------------------------------------------------------- #
    #  Parallel scheduler: guard-rail on question + chunk queries
    # --------------------------------------------------------------------- #
    async def _run_parallel(
        self, full_text: str, history: List[dict[str, str]], user_question: str
    ) -> str:
        chunks = self._chunk_context(full_text)

        guardrail_task = asyncio.create_task(
            self.llm_client.GuardrailsDetection.apply_guardrail(user_question, history)
        )

        chunks_task = asyncio.create_task(
            self.llm_client.run_on_chunks(
                chunks, history, user_question, self.SYS_TPL, self.USER_TPL
            )
        )

        done, _ = await asyncio.wait(
            [guardrail_task, chunks_task], return_when=asyncio.FIRST_COMPLETED
        )

        # 1️⃣ guard-rails finished first
        if guardrail_task in done:
            gr = self.llm_client.safe_load_json(guardrail_task.result())
            if gr["Status"] == "Reject":
                chunks_task.cancel()
                return self.llm_client.GuardrailsDetection.get_response_for_guardrail(gr, "Rejected")
            answers = await chunks_task
        else:
            # 2️⃣ chunk answers finished first
            answers = chunks_task.result()
            gr = self.llm_client.safe_load_json(await guardrail_task)
            if gr["Status"] == "Reject":
                return self.llm_client.GuardrailsDetection.get_response_for_guardrail(gr, "Rejected")

        # summarise across all chunks
        return await self.llm_client.summarize_answers(answers, user_question)

    # --------------------------------------------------------------------- #
    #  MAIN entry – called by API layer
    # --------------------------------------------------------------------- #
    async def _process_request_async(
        self, session_id: str, user_question: str, file_path: Optional[str] = None
    ) -> str:
        try:
            # Greeting (only first turn)
            if not session_exists(session_id):
                greet = self.llm_client.check_greeting(user_question)
                if greet:
                    return json.dumps(greet)

            if not file_path:
                return json.dumps({"session_id": session_id, "error": "File path required"})

            full_text = self.file_handler.process_files_from_api_sequential(file_path, start_seq=1)
            history   = get_chat_history(session_id)
            tokens    = count_tokens_for_context(full_text)

            # ── Normal vs big-document path
            if tokens <= 50_000:
                messages = [
                    {"role": "system", "content": self.SYS_TPL.format(context=full_text)},
                    *history,
                    {"role": "user", "content": self.USER_TPL.format(user_question=user_question)},
                ]
                resp, _ = await self.llm_client.execute_chat_with_guardrail(
                    messages, user_question, history
                )
                return resp

            # big doc ⇒ parallel chunk flow
            return await self._run_parallel(full_text, history, user_question)

        except Exception as exc:
            self.logger.error(traceback.format_exc())
            return json.dumps({"error": str(exc)})


def record_step_output_sync(step_number: int, output):
    """
    Thread-safe helper that works whether or not an event loop is running.
    """
    coro = record_step_output(step_number, output)
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        # no loop → safe to start one
        asyncio.run(coro)
    else:
        # loop already running → fire-and-forget task
        asyncio.create_task(coro)
