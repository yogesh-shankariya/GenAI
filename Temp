"""
LLMClient – handles all LLM-side interaction, guard-rails, and (now) the
chunk-level summarisation flow used when a document exceeds 50 000 tokens.
"""

from __future__ import annotations

import asyncio
import json
import os
import re
import unicodedata
from typing import Dict, List, Any, Iterable

import logging as app_logger
from retry import retry

from chatbot.src.utils.horizon_api import getAuthToken, chats
from chatbot.src.utils.file_and_session_manager import FileHandler
from chatbot.src.utils.prompt_loader import load_prompt
from chatbot.src.services.responsible_ai import GuardrailsDetection
from constants import Constant

# paths to prompts / support files
_current_dir = os.path.dirname(__file__)
json_converter_prompt_path      = os.path.join("prompts", "other", "json_prompt.yaml")
guardrail_prompt_path           = os.path.join("prompts", "responsible_ai", "guardrails_prompt.yaml")
greeting_json_path              = os.path.join(_current_dir, "greetings.json")
question_recommendation_path    = os.path.join("prompts", "other", "question_recommendation.yaml")

# ──────────────────────────────────────────────────────────────────────────────
#  Prompt for merging partial answers (chunk summaries → final answer)
# ──────────────────────────────────────────────────────────────────────────────
CHUNK_SUMMARY_PROMPT = """
You are a careful, evidence-based clinical assistant.

You will receive:
• The current user question.
• A numbered list of partial answers produced from overlapping document chunks.

Task:
1. Merge the partial answers into ONE coherent answer.
2. Preserve every medically relevant detail; do **not** omit facts.
3. If two partial answers disagree, include both views and mark the conflict.
4. Do NOT add new information.

Return only valid JSON (no markdown):

{
  "content": "<final answer text>",
  "references": [],
  "metrics": [],
  "recommended_questions": []
}
""".strip()


# ═════════════════════════════════════════════════════════════════════════════
class LLMClient:  # base class in your project – fully rewritten here
# ═════════════════════════════════════════════════════════════════════════════
    # --------------------------------------------------------------------- #
    def __init__(self, config: Dict[str, Any], logger):
        self.config  = config
        self.logger  = logger

        self.greeting_json_path = greeting_json_path
        self.chats              = chats
        self.safe_load_json     = FileHandler.safe_load_json

        # prompt fragments
        self.guardrail_prompt                  = load_prompt(guardrail_prompt_path)["PROMPT"]
        jc_prompt                              = load_prompt(json_converter_prompt_path)
        self.json_converter_system_prompt      = jc_prompt["system"]
        self.json_converter_user_prompt        = jc_prompt["user"]
        qr_prompt                              = load_prompt(question_recommendation_path)
        self.question_recommendation_system_prompt = qr_prompt["system_prompt"]
        self.question_recommendation_user_prompt   = qr_prompt["user_prompt"]

        # guard-rails
        self.GuardrailsDetection = GuardrailsDetection(config, self.logger)

    # --------------------------------------------------------------------- #
    #  Greeting helpers
    # --------------------------------------------------------------------- #
    def _normalize(self, txt: str) -> str:
        txt = unicodedata.normalize("NFKD", txt).encode("ascii", "ignore").decode("utf-8")
        txt = re.sub(r"[^\w\s]", "", txt)
        return re.sub(r"\s+", " ", txt.strip().lower())

    def get_greeting_data(self) -> Dict[str, Any]:
        with open(self.greeting_json_path, "r", encoding="utf-8") as fh:
            return json.load(fh)

    def check_greeting(self, user_input: str) -> dict | None:
        data = self.get_greeting_data()
        if self._normalize(user_input) in map(self._normalize, data["greeting_keywords"]):
            return {
                "status": 200,
                "data": {
                    "content": data["generic_response"],
                    "references": [],
                    "metrics": [],
                    "recommended_questions": [],
                },
            }
        return None

    # --------------------------------------------------------------------- #
    #  Core single-call with conversation memory  (used by chunk calls)
    # --------------------------------------------------------------------- #
    @retry(tries=3, delay=1, backoff=2, logger=app_logger)
    async def query_with_memory(self, messages: List[Dict[str, str]]) -> str:
        token = getAuthToken(
            Constant.HORIZON_CLIENT_ID,
            Constant.HORIZON_CLIENT_SECRET,
            Constant.HORIZON_GATEWAY,
        )
        if not token:
            self.logger.error("Q&A : Authentication Failed (token empty)")
            return ""

        completion = self.chats(
            authToken=token,
            address=Constant.HORIZON_GATEWAY,
            messages=messages,
        )
        if completion:
            return json.loads(completion.text)["message"]["content"]

        self.logger.error("No response from Q&A LLM.")
        return ""

    # --------------------------------------------------------------------- #
    #  Existing guard-rail wrapper (unchanged)
    # --------------------------------------------------------------------- #
    async def execute_chat_with_guardrail(self, messages, user_question, history):
        guardrail_task = asyncio.create_task(
            self.GuardrailsDetection.apply_guardrail(user_question, history)
        )
        chat_task = asyncio.create_task(self.query_with_memory(messages))

        while True:
            done, _ = await asyncio.wait(
                [guardrail_task, chat_task], return_when=asyncio.FIRST_COMPLETED
            )

            if guardrail_task in done:
                gr_resp = self.safe_load_json(guardrail_task.result())
                if gr_resp["Status"] == "Reject":
                    chat_task.cancel()
                    return self.GuardrailsDetection.get_response_for_guardrail(gr_resp, "Rejected")
            if chat_task in done:
                return chat_task.result(), "Approved"
            await asyncio.sleep(0.1)

    # --------------------------------------------------------------------- #
    #  New: call one LLM request per chunk (NO guard-rails)
    # --------------------------------------------------------------------- #
    async def _one_chunk_call(
        self,
        chunk: str,
        history: List[Dict[str, str]],
        user_question: str,
        sys_tpl: str,
        user_tpl: str,
    ) -> str:
        msgs = [
            {"role": "system", "content": sys_tpl.format(context=chunk)},
            *history,
            {"role": "user", "content": user_tpl.format(user_question=user_question)},
        ]
        return await self.query_with_memory(msgs)

    async def run_on_chunks(
        self,
        chunks: Iterable[str],
        history: List[Dict[str, str]],
        user_question: str,
        sys_tpl: str,
        user_tpl: str,
    ) -> List[str]:
        coros = [
            self._one_chunk_call(chunk, history, user_question, sys_tpl, user_tpl)
            for chunk in chunks
        ]
        return await asyncio.gather(*coros)

    # --------------------------------------------------------------------- #
    #  New: merge partial answers into one final JSON answer
    # --------------------------------------------------------------------- #
    async def summarize_answers(self, answers: List[str], user_question: str) -> str:
        numbered = "\n".join(f"{i+1}. {a}" for i, a in enumerate(answers))
        msgs = [
            {"role": "system", "content": CHUNK_SUMMARY_PROMPT},
            {"role": "user", "content": f"Question:\n{user_question}\n\nPartialAnswers:\n{numbered}"},
        ]
        final, _ = await self.execute_chat_with_guardrail(msgs, user_question, history=[])
        return final
