"""
FAISS vector store integration using LangChain.

This module wraps LangChain's FAISS integration to provide a simple
interface for constructing, updating and querying FAISS indices over
lists of ``Document`` objects.  Each collection corresponds to a
separate FAISS index stored either in memory or on disk.  Documents
are represented by LangChain's ``Document`` class, which stores both
the text content and arbitrary metadata.

Key functions provided here:

* ``create_faiss_store_from_documents`` – build a new FAISS index
  from a list of ``Document`` objects and return the vector store.

* ``add_documents`` – add additional documents to an existing vector
  store.

* ``query`` – perform a similarity search against a store for a
  query string and return the most similar documents with scores.

* ``save_store`` and ``load_store`` – persist a vector store to
  disk and load it back later.

These helpers use the LangChain ``FAISS`` class under the hood.  See
the official documentation for more details: https://python.langchain.com/docs/integrations/vectorstores/faiss/

Example usage::

    from langchain.docstore.document import Document
    from embedding import create_embedder
    from vector_store import (
        create_faiss_store_from_documents,
        add_documents,
        query,
        save_store,
    )

    # Prepare some sample documents
    docs = [
        Document(page_content="Hello world", metadata={"id": 1}),
        Document(page_content="Another document", metadata={"id": 2}),
    ]

    # Create an embedder (using BGE-M3 via HuggingFace)
    embedder = create_embedder(model_name="BAAI/bge-m3")

    # Build the vector store
    store = create_faiss_store_from_documents(docs, embedder)

    # Query the store
    results = query(store, "world", k=2)
    for doc, score in results:
        print(doc.page_content, score)

    # Persist the store to disk
    save_store(store, "alerts_index")
    # Later reload it:
    store2 = load_store("alerts_index", embedder)
"""

from __future__ import annotations

from typing import List, Tuple, Optional

try:
    # ``langchain_community.vectorstores.faiss`` is the new location for FAISS in LangChain >= 0.1
    from langchain_community.vectorstores.faiss import FAISS  # type: ignore
except ImportError:
    try:
        # Fallback for older versions of LangChain
        from langchain.vectorstores.faiss import FAISS  # type: ignore
    except ImportError as e:
        raise ImportError(
            "FAISS vector store could not be imported. Please install langchain-community and faiss-cpu."
        ) from e

try:
    from langchain.docstore.document import Document
except ImportError as e:
    raise ImportError(
        "Could not import Document from langchain. Please ensure langchain is installed."
    ) from e

from embedding import HFEmbedder  # type: ignore


def create_faiss_store_from_documents(
    documents: List[Document],
    embedder: HFEmbedder,
) -> FAISS:
    """Create a FAISS vector store from a list of documents.

    Args:
        documents: A list of ``Document`` instances containing the text and metadata.
        embedder: An embedder implementing ``embed_documents`` and ``embed_query``.

    Returns:
        A LangChain ``FAISS`` vector store initialised with the provided documents.

    This uses the static method ``FAISS.from_documents`` under the hood
    (see LangChain docs).  The resulting store can be saved to disk
    using ``save_store`` and reloaded with ``load_store``.
    """
    # LangChain's FAISS.from_documents will handle embedding and indexing
    return FAISS.from_documents(documents, embedder.embedder)


def add_documents(store: FAISS, documents: List[Document]) -> None:
    """Add documents to an existing FAISS vector store.

    Args:
        store: The FAISS vector store to update.
        documents: A list of documents to add.

    Raises:
        ValueError: If the store is not a FAISS instance.
    """
    if not isinstance(store, FAISS):
        raise ValueError("store must be a FAISS vector store")
    store.add_documents(documents)


def query(store: FAISS, query_text: str, k: int = 5) -> List[Tuple[Document, float]]:
    """Query the FAISS store for the top-k most similar documents.

    Args:
        store: The FAISS vector store to query.
        query_text: The raw text of the query.
        k: Number of top documents to return.

    Returns:
        A list of tuples ``(document, score)`` where ``score`` is the
        similarity score (higher is more similar).  The length of the
        returned list will be ``min(k, len(store))``.

    This function calls the underlying ``similarity_search_with_score``
    method provided by LangChain's ``FAISS`` implementation.
    """
    results: List[Tuple[Document, float]] = store.similarity_search_with_score(query_text, k=k)
    return results


def save_store(store: FAISS, path: str) -> None:
    """Persist a FAISS vector store to disk.

    Args:
        store: The FAISS vector store to save.
        path: The filesystem path (directory) where the index will be
            stored.  If the directory does not exist, it will be
            created.  Note that the path should be unique for each
            collection, as it will create multiple files with a
            naming scheme based on this path.
    """
    store.save_local(path)


def load_store(path: str, embedder: HFEmbedder) -> FAISS:
    """Load a FAISS vector store from disk.

    Args:
        path: The directory where the vector store was saved.
        embedder: An embedder used for queries.  The same model as
            used when creating the index should be provided to ensure
            correct similarity measurements.

    Returns:
        A ``FAISS`` vector store instance loaded from disk.
    """
    return FAISS.load_local(path, embedder.embedder)
