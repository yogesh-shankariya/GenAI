# client.py

import httpx
from typing import List, Dict


API_URL = "https://api.llm.com/v2/tool/chats"


async def run_tool_chat(
    token: str,
    tools: List[Dict],
    messages: List[Dict],
    *,
    qos: str = "accurate",
    preview: bool = False,
    reasoning: bool = False,
    timeout: int = 30
) -> Dict:
    """
    Send a single request to the Custom Tool-Chat endpoint and return
    the JSON response.

    Parameters
    ----------
    token : Bearer token string.
    tools : Tools section of the payload (list of dicts).
    messages : Message history (list of dicts).
    qos : 'accurate', 'fast', etc. (default: 'accurate')
    preview : Whether to enable preview mode. (default: False)
    reasoning : Whether to ask for chain-of-thought. (default: False)
    timeout : Network timeout in seconds. (default: 30)
    """

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {token}",
    }

    params = {
        "qos": qos,
        "preview": str(preview).lower(),
        "reasoning": str(reasoning).lower(),
    }

    payload = {"tools": tools, "messages": messages}

    async with httpx.AsyncClient(timeout=timeout, verify=False) as client:
        resp = await client.post(API_URL, headers=headers, params=params, json=payload)
        resp.raise_for_status()  # raises if status >= 400
        return resp.json()


# Dev_11_01_2026.ipynb (code cells shown in images)

import asyncio, os


# --- 2.1 Define the tool schema ---------------------------------------------

weather_tool = [
    {
        "type": "adhoc",
        "name": "getWeather",
        "description": "Returns a weather summary for a location.",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": (
                        "The city and optional country/state "
                        "(e.g., 'London, UK' or 'New York City, NY')."
                    ),
                }
            },
            "required": ["location"],
            "additionalProperties": False,
        },
    }
]


# --- 2.2 Define a stub implementation of the tool ---------------------------

def get_weather(location: str) -> str:
    # In real code you'd call a weather API here.
    return "hot and muggy"


# --- 2.3 Conversation loop --------------------------------------------------

async def main():
    token = 'a8lp8TNyjp57mpjoLsk98qQoRVIVUrxt'  # or hard-code for quick tests

    # Step-1 : user asks a question
    messages = [
        {"type": "message", "role": "user", "content": "What is the weather in Chicago, IL"}
    ]

    # Call the LLM
    assistant_resp = await run_tool_chat(token, weather_tool, messages)
    print("Assistant response =>", assistant_resp)

    # Step-2 : inspect whether a tool call was requested
    tool_call_msg = next((m for m in assistant_resp["messages"] if m["type"] == "tool_call"), None)
    if not tool_call_msg:
        raise RuntimeError("No tool_call message found!")

    # Step-3 : execute the tool locally
    tool_output = get_weather(**tool_call_msg["input"])

    # Step-4 : add tool_call + tool_result to the conversation
    messages.extend(
        [
            tool_call_msg,
            {
                "type": "tool_result",
                "role": "user",
                "id": tool_call_msg["id"],
                "result": tool_output,
            },
        ]
    )

    # Step-5 : ask the model to finish the answer
    final_resp = await run_tool_chat(token, weather_tool, messages)

    # The last message should contain the natural-language answer
    print("Final answer =>", final_resp["messages"][-1]["content"])


await main()
