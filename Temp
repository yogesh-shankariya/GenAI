from typing import Dict, Any


def attach_validation_metrics(
    llm_response: Dict[str, Any],
    faithfulness: Dict[str, Any],
    relevance: Dict[str, Any],
) -> Dict[str, Any]:
    """
    Enrich an existing LLM response (already containing status = 200 and
    `data.content` / `data.reference`) with evaluation scores.

    Parameters
    ----------
    llm_response : dict
        The JSON-serialisable object returned from your structured-answer step,
        e.g. {"status": 200, "data": {"content": "...", "reference": [...]}}
    faithfulness : dict
        Output produced by ResponseValidationScoreCalculator.get_faithfulness_score,
        e.g. {"score": 0.93, "justification": "..."}
    relevance : dict
        Output produced by ResponseValidationScoreCalculator.get_relevance_score,
        e.g. {"score": 0.87, "justification": "…"}.

    Returns
    -------
    dict
        Same object, now with a `data["metrics"]` section:

        {
            "status": 200,
            "data": {
                "content": "...",
                "reference": [...],
                "metrics": {
                    "faithfulness": {"score": …, "justification": …},
                    "relevance"   : {"score": …, "justification": …}
                }
            }
        }
    """
    # Ensure nested keys exist
    data_block = llm_response.setdefault("data", {})

    # Attach or overwrite metrics
    data_block["metrics"] = {
        "faithfulness": faithfulness,
        "relevance": relevance,
    }

    return llm_response
